{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity 딥러닝 강좌 - L4_Deep Models for Text and Sequences Videos\n",
    "\n",
    "- Udacity 딥러닝 동영상 강좌 => https://www.udacity.com/course/deep-learning--ud730\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지는 고정 길이(변수 개수가 고정)의 데이터를 이용한 딥러닝 알고리즘을 알아보았음.\n",
    "- 시계열 데이터(가변 길이)를 다룰 수 있는 딥러닝 알고리즘에 대해서 알아보자.\n",
    "- 시계열 데이터로 흔히 볼 수 있는 것이 텍스트(문서) 데이터임.\n",
    "\n",
    "### Text 를 다루는 방법에 대해서 알아보자.\n",
    "- 텍스트를 분류하기 위해서는 해당 문서에서만 나오는 단어를 찾는 것이 중요합니다.\n",
    "    - 예) duh(감탄사) -> 알 수 없음.\n",
    "    - 예) retinopathy( 망막병증 ) -> 의학 문서\n",
    "\n",
    "![](rnn_theory_01.jpg)\n",
    "\n",
    "- 단어들의 의미를 연산이 가능한 형태로 수치로 변환이 필요합니다.\n",
    "    - 비슷한 의미를 갖는 단어는 비슷한 수치로 표현되어야 함.\n",
    "    - 단어가 수치 변환되는 방법은 비지도 학습으로 이루어짐.\n",
    "        - <font color='red'>유사한 단어는 유사한 상황에서 발생하는 경향이 있다는 것을 이용함.</font>\n",
    "        - 예) 문서를 학습할때, \"고양이(cat)가 쥐를 잡는다\"는 문장과 \"새끼고양이(kitten)가 쥐를 잡는다\"는 문장이 나왔다면, cat과 kitten은 유사한 단어라고 생각할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유사 단어를 서로 근접하게 vector로 맵핑하는 것을 **Embedding** 이라고 함.\n",
    "    - 유사하지 않은 단어를 멀리 떨어지게 됨.\n",
    "    - 희소성의 문제도 해결됨\n",
    "    - 한번 학습을 하면, 새로운 데이터에 대해서 다시 학습이 필요없음.\n",
    "        - 신조어가 나와도 그 단어의 주변에 있는 단어를 보면 어떤 의미를 갖는지 알 수 있음.=> 자연어 처리 및 검색 엔진에 사용됨.\n",
    "        - 예) 고양이와 비슷한 단어들은 특정 패턴으로 일반화됨.\n",
    "![](rnn_theory_03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Embeddings하는 방법으로 Word2Vec에 대해서 알아보자.\n",
    "- 문장에서 각각의 단어를 embedding하도록 맵핑함.\n",
    "- 처음에는 램덤하게 중심 단어를 선택해서 시작함.\n",
    "    - 중심 단어 주변의 WINDOW(관찰할 데이터의 제한된 크기)만큼 단어를 고려함.\n",
    "    - 중심 단어는 인접한 단어를 이용해서 로지스틱 회귀모형으로 설명할 수 있음.\n",
    "    - 예) 확률(FOX) ~ a1 x Quick + a2 x Brown + a3 x Jumps + a4 x Over "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec을 수행하면 유사 단어들을 clustering이 되어, 유사단어는 인접하게 됨.\n",
    "- 결과를 2차원으로 줄이는 방법\n",
    "    - PCA : 데이터 유실이 많아서 의미있는 결과가 나오지 않음.\n",
    "    - [t-SNE(t-distributed stochastic neighbor embedding)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) : 데이터의 이웃한 구조를 보존하면서 2차원으로 투영가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2Vec의 2가지 상세 기술에 대해 알아보자.\n",
    "\n",
    "- 1) 인접 계산 방식을 L2 대신에 Coise distance를 사용함\n",
    "    - embedding vector을 정규화 하기 위해서 => 거리범위를 0 ~ 1로 표현함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 2) word2vec는 선형모형에 의해서 결과가 나오는데, 사전에 단어가 많기 때문에 계산이 비효율적임.\n",
    "    - 1인 확률을 갖는 단어와 0인 확률을 갖는 단어의 일부분을 샘플링해서 각각의 단어에 대해서 negative target을 샘플링하는 방법.\n",
    "    - 이를 Sampled Softmax라고 함.\n",
    "    - 성능에 부담이 없이 빠르게 수행됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_07.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3) Word2Vec 문제\n",
    "    - 문제를 풀어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_08.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec를 사용하면 아래와 같이 수치적 표현이 가능함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_09.jpg)\n",
    "![](rnn_theory_10.jpg)\n",
    "![](rnn_theory_11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding을 학습할 수 있는 딥러닝 알고리즘인 RNN에 대해서 알아보자.\n",
    "- RNN이 필요한 이유??\n",
    "    - 위와 같은 결과를 나오기 위해서는 아주 많은 데이터가 필요함.\n",
    "    - 문장의 길이는 고정이 아니고, 가변이라서 기존의 neural network으로는 불가능함.\n",
    "- RNN의 기반 아이디어 \n",
    "    - CNN에서는 공간 상에 있는 feature에 대해서 동일한 가중치(W)를 주고, RNN에서는 time에 대해서 동일한 가중치를 줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequence데이터에서는 과거 고려하기 위해서 어느 시점이전의 일어난 사건을 다룸.\n",
    "- 그래서, 이전의 분류기의 상태값을 사용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_13.jpg)\n",
    "![](rnn_theory_14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN에서의 Backpropagation\n",
    "  - 모든 시간(t)에 대해서 모든 미분값들이 공유되는 가중치(W)에 적용이 됨\n",
    "  - Stochastic gradient descent에서는 바쁜 결과가 나옴.\n",
    "      - Exploding gradient : 가중치가 무한대로 증가하는 문제\n",
    "      - Vanishing gradient : 가중치가 사라지는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_15.jpg)\n",
    "![](rnn_theory_16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploding gradient는 gradient clipping 라는 방법을 해결\n",
    "   - gradient가 너무 커지면, 각 단계에서 수축시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanishing gradient는 최근의 사건은 잘 기억하고 먼 과거의 사건은 기억하지 않는 방법으로 해결함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM( Long Short-Term Memory )에 대해서 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN은 단순하고 반복적인 구조로 과거로부터 입력값(PAST), 새로운 입력값(X), 새로운 결과(Y), 다음과 연결값(Future)을 가지고 있음.\n",
    "  - 중앙에는 전형적은 layer 구조를 갖음.\n",
    "- LSTM이라는 작은 모듈로 교체함.\n",
    "  - LSTM은 RNN에서 띄어낼수 있고, RNN 아키텍처와 동일한 구조\n",
    "  - Vanishing gradient 문제를 훌륭하게 해결해줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_18.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM의 기능 \n",
    "   - 1)입력 데이터를 메모리에 쓰는 기능(Write)\n",
    "   - 2)메모리에서 데이터를 읽기 기능(Read)\n",
    "   - 3)메모리에 있는 데이터를 지우는 기능(Forget)\n",
    "   - 4)Write, Read, Forget을 여부를 결정하는 gate를 각각 구성함.\n",
    "   \n",
    "- LSTM의 특성  \n",
    "   - 1)각각의 gate에서 0을 곱하면, gate가 닫힘, 1을 곱하면 gate가 열림.\n",
    "   - 2)gate에서 0 ~ 1사이값이면 부분값이 적용됨.\n",
    "   - 3) **gate의 곱셈계수가 연속함수이고 미분가능하면, 도함수가 존재하고 그러면, back progate을 통해서 곱셈계수를 최적화할 수 있음 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_19.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gate에서의 back progation 방법\n",
    "    - 각각의 gate의 대한 gate value들은 입력(X)에 대한 간단한 로직스틱 회귀에 조절됨.\n",
    "    - 각각의 gate은 $ W_o, W_i, W_{r-1}, W_F, W_r $ 값들은 공유됨 \n",
    "    - 추가된 hyerbic Tension은 출력을 -1 과 1 사이로 유지해주는 역할.\n",
    "    - 복잡해보이지만, 다섯개의 수식으로 표현됨. \n",
    "\n",
    "- LSTM의 장점\n",
    "    - 각각의 gate의 가중치들은 연속이고 미분가능해서 쉽게 최적화를 할 수 있음.\n",
    "    - 이렇게 단순한 구조로 필요여부에 따라서 gate에서 메모리를 관리를 할 수 있음.\n",
    "    - 그러므로, 최적화는 쉽고 Vanishing gradient를 제거할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_20.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM의 Reqularization\n",
    "    - L2\n",
    "    - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN으로 모델을 만든후에 Sequence를 생성하는 법에 대해서 알아보자.\n",
    "- 일부 sequence가 있고 어느 시점(t)에서 RNN 모델로부터 다음 단어또는 문자을 예측할 수 있고, 예측된 분포로부터 확률이 가장 높은 것을 선택할 수 있음.\n",
    "- 이런 식으로 반복적으로 계속 진행할 수 있음.\n",
    "- 최적화된 sequence가 찾기 위해서는 greedy 방식을 이용하며,\n",
    "- 이 방법은 너무 비효율적임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_22.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bean Search\n",
    "   - 한 시점(t)마다 샘플링하는 대신에 여러 시점에서 샘플링을 시도함.\n",
    "   - 예) 확률이 높은 A, O을 선택하고, 이후의 단계에서 계속 예측을 시도함. 그리고 이중에서 전체확률이 가장 높은sequence을 선택함.\n",
    "   - 이 방법은 단계가 진행할수록 계산양이 기하급수적으로 증가하므로, 각각의 시점에서 가장 가능성이 높은 몇몇 후보 서열만 남기고 계산을 진행함. => Bean Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_21.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN을 활용한 응용 방법을 알아보자.\n",
    "\n",
    "- Deep learning 모델들은 레고와 같이 가지고 놀 수 있음.\n",
    "- 모델들을 서로 섞고 서로 붙인후에 back prop로 최적화할 수 있음\n",
    "- 이를 활용해서\n",
    "- 가변길이를 입력받아 고정길이를 출력하는 모델과  고정길이를 입력받아서 가변길이를 출력하는 모델을 합칠 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_23.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래와 같이 자동번역기를 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래와 같이 음성 인식 시스템을 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN과 RNN을 합치면, 이미지를 입력받아서 단어의 sequence을 출력받을 수 있으므로, 이미지에 대한 captions을 생성할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rnn_theory_26.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
