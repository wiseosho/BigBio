{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.Building a Classification Model with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류(Classification)는 범주 또는 종류로 구분하는 것\n",
    "- 분류의 종류로는 이진분류와 멀티클래스 분류가 있음.\n",
    "- 분류는 학습용 데이터에 정답 또는 관심있는 값을 지정하고 모델을 학습시키는 지도학습(supervised learning)임.\n",
    "- 군집과 차이는 ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A simple binary classification problem](Classification_01.jpg \"A simple binary classification problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A simple binary classification problem](Classification_02.jpg \"A simple binary classification problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류의 용도\n",
    "- 1) 인터넷 사용자가 광고를 클릭할지 여부를 예측\n",
    "- 2) 사기 거래 여부를 예측\n",
    "- 3) 부도 여부를 예측\n",
    "- 4) images, video, sounds의 분류\n",
    "- 5) 새로운 뉴스 또는 페이지의 분류\n",
    "- 6) 스팸메일 및 해킹 여부를 판단\n",
    "- 7) 고객의 등급 분류\n",
    "- 8) 고객 이탈 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이번 장의 목표\n",
    "- 1) SparkML에서 사용할 수 있는 분류모델의 종류을 논의\n",
    "- 2) raw 데이터로부터 알맞는 특징을 추출하는 방법 알아보기\n",
    "- 3) 분류모델을 학습하는 방법 알아보기\n",
    "- 4) 학습된 모델을 이용해서 예측기 만들기\n",
    "- 5) 예측기의 성능을 측정할 수 있는 표준적인 평가방법을 알아보기\n",
    "- 6) 여러가지 특성값 추출방법을 이용해서 모델의 성능을 향상법을 알아보기\n",
    "- 7) 모델 성능에 관련된 튜닝 파라메타이 영향도를 알아보고, 최적의 모델 파라메타를 선택하기 위한 교차분석법을 배워보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Types of classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) 선형모델 : 매우 큰 데이터셋에 적용하기 쉬음.\n",
    "- 2) 나무모델 : 스케일업에 다소 어려움이 있지만, 강력한 비선형 모델을 제공\n",
    "- 3) 나이브 베이즈모델 : 더 단순하지만, 효과적인 학습과 병렬처리가 쉬음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이진분류기 : 선형모델, 나무모델, 나이브베이즈 모델을 SparkML에서 지원\n",
    "- 멀티클래스 분류기 : 나무모델, 나이브베이즈 모델을 SparkML에서 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력변수들을 단순한 선형 함수를 사용해서 목표변수( 종속변수 )를 예측하는 모형\n",
    "- 행렬을 이용한 중회귀모형\n",
    "![행렬을 이용한 중회귀모형](regress_01.jpg \"행렬을 이용한 중회귀모형\")\n",
    "![행렬을 이용한 중회귀모형](regress_02.jpg \"행렬을 이용한 중회귀모형\")\n",
    "![행렬을 이용한 중회귀모형](regress_03.jpg \"행렬을 이용한 중회귀모형\")\n",
    "![행렬을 이용한 중회귀모형](regress_04.jpg \"행렬을 이용한 중회귀모형\")\n",
    "\n",
    "- 로지스틱 회귀모형 \n",
    "![로지스틱 회귀모형](Classification_03.jpg \"로지스틱 회귀모형\")\n",
    "![로지스틱 회귀모형](Classification_04.jpg \"로지스틱 회귀모형\")\n",
    "![로지스틱 회귀모형](Classification_05.jpg \"로지스틱 회귀모형\")\n",
    "![로지스틱 회귀모형](Classification_06.jpg \"로지스틱 회귀모형\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Linear support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률적인 모델이 아닌, 모델평가를 긍정/부정 인지를 기반하여 분류을 예측함.\n",
    "- SVM의 손실함수는 hinge loss라고 하면 아래와 같음.\n",
    "![loss function for SVM](Classification_07.jpg \"loss function for SVM\")\n",
    "\n",
    "- Decision functions for logistic regression and linear SVM for binary classification\n",
    "![Decision functions for logistic regression and linear SVM for binary classification](Classification_08.jpg \"Decision functions for logistic regression and linear SVM for binary classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. The naïve Bayes model\n",
    "\n",
    "- 주어진 학습용 데이터로부터 각각의 분류의 가장 가능도가 높은 분포개형을 계산하고, 이를 기반으로 분류\n",
    "\n",
    "![The naïve Bayes model](Classification_09.jpg \"The naïve Bayes model\")\n",
    "![The naïve Bayes model](Classification_10.jpg \"The naïve Bayes model\")\n",
    "![The naïve Bayes model](Classification_11.jpg \"The naïve Bayes model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아주 복잡한 비선형 패턴이나 특성치들의 상호작용을 잡아낼 수 있는 강력하고, 비확률적인 기법\n",
    "- 집에 있을지, 야외에 놀러나갈지를 결정 방식을 도식화\n",
    "![Decision trees](Classification_12.jpg \"Decision trees\")\n",
    "\n",
    "- 나이브베이즈 모형과 비교해보자. \n",
    "![Decision trees](Classification_13.jpg \"Decision trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracting the right features from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특성치(독립변수)는 숫자의 vector형식으로 주로 다루어지고, 분류기나 회귀는 지도학습이기 때문에 feature vector와 target value가 필요함.\n",
    "- Spark MLlib에서는 LabeledPoint 클래스를 지원함.\n",
    "- case class LabeledPoint(label: Double, features: Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Extracting features from the Kaggle/StumbleUpon evergreen classification dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋은 StumbleUpon에서 주어짐.\n",
    "- 자신들의 웹 컨텐츠 추천 페이지에 있는 웹페이지에서 단기간만 살아있을 것(non-evergreen)과 오래동안 인기(evergreen)가 있을것을 분류하는 문제임.\n",
    "- http://www.kaggle.com/c/stumbleupon/data\n",
    "- train.tsv 파일을 다운로드 \n",
    "- train.tsv의 각각의 필드 설명\n",
    "![The following table includes field descriptions for train.tsv](Classification_14.jpg \"The following table includes field descriptions for train.tsv\")\n",
    "\n",
    "\n",
    "\n",
    "- 헤더가 포함되어 있어서 첫번째 라인을 제거함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sed 1d train.tsv > train_noheader.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark shell을 시작하고 학습데이터를 읽어서 한줄씩 탭으로 자름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin/spark-shell --master local[*] --driver-memory 4G\n",
    "\n",
    "val rawData = sc.textFile(\"file:///home/bigbio/data/train_noheader.tsv\")\n",
    "val records = rawData.map(line => line.split(\"\\t\"))\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1번째와 2번째 컬럼은 URL과 페이지ID\n",
    "- 다음 2개의 컬럼은 문자열 데이터 \n",
    "- 다음 22개의 컬럼은 숫자 또는 범주형 데이터\n",
    "- 마지막 컬럼은 목표값으로 1이면 evergreen, 0이면 non-evergreen\n",
    "\n",
    "\n",
    "- 이진 범주형 데이터에 대해서, 이미 1-of-k encoding 을 가지고 있어서 추가적인 변수 추출은 하지 않음\n",
    "- 데이터중에서 ? 문자열을  0.0으로 할당함.\n",
    "- label은 정수형, features은 Array[Double] 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "val data = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val features = trimmed.slice(4, r.size - 1).map(d => if (d ==\"?\") 0.0 else d.toDouble)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "data.cache\n",
    "val numData = data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나이브베이즈 모델에서는 음수값에 대해서는 에러를 발생하기 때문에 음수를 0.0으로 변환하는 버전을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nbData = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val features = trimmed.slice(4, r.size - 1).map(d => if (d ==\"?\") 0.0 else d.toDouble).map(d => if (d < 0) 0.0 else d)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "val numNbData = nbData.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression, SVM, naïve Bayes, and a decision tree 각각을 모두 훈련시킴.\n",
    "- 최고의 파라메터 세팅을 위해서 평가기법을 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n",
    "import org.apache.spark.mllib.classification.SVMWithSGD\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "val numIterations = 10\n",
    "val maxTreeDepth = 5\n",
    "\n",
    "val lrModel = LogisticRegressionWithSGD.train(data, numIterations)\n",
    "\n",
    "val svmModel = SVMWithSGD.train(data, numIterations)\n",
    "\n",
    "val nbModel = NaiveBayes.train(nbData)\n",
    "\n",
    "val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dataPoint = data.first\n",
    "val prediction = lrModel.predict(dataPoint.features)\n",
    "val trueLabel = dataPoint.label\n",
    "\n",
    "val predictions = lrModel.predict(data.map(lp => lp.features))\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating the performance of classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![accuracy precision recall](Classification_15.jpg \"accuracy  precision recall\")\n",
    "\n",
    "- precision와  recall는 역의 관계를 가짐\n",
    "![precision recall](Classification_16.jpg \"precision recall\")\n",
    "\n",
    "- Precision-recall curve\n",
    "![Precision-recall curve](Classification_20.jpg \"Precision-recall curve\")\n",
    "\n",
    "- precision와  recall의 조화평균을 F1 값이라고 함.\n",
    "![precision recall](Classification_17.jpg \"precision recall\")\n",
    "\n",
    "![ROC](Classification_18.jpg \"ROC\")\n",
    "\n",
    "![AUC](Classification_19.jpg \"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Accuracy and prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 로지스틱 선형분류의 정확도 \n",
    "val lrTotalCorrect = data.map { point =>\n",
    "    if (lrModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracy = lrTotalCorrect / data.count\n",
    "\n",
    "// SVM의 정확도\n",
    "val svmTotalCorrect = data.map { point =>\n",
    "    if (svmModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val svmAccuracy = svmTotalCorrect / numData\n",
    "\n",
    "// naïve Bayes 의 정확도\n",
    "val nbTotalCorrect = nbData.map { point =>\n",
    "    if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val nbAccuracy = nbTotalCorrect / numData\n",
    "\n",
    "// decision tree의 정확도\n",
    "val dtTotalCorrect = data.map { point =>\n",
    "    val score = dtModel.predict(point.features)\n",
    "    val predicted = if (score > 0.5) 1 else 0\n",
    "    if (predicted == point.label) 1 else 0\n",
    "}.sum\n",
    "val dtAccuracy = dtTotalCorrect / numData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Precision and recall\n",
    "\n",
    "- sparkMLlib에서는 이진분류기의 PR값과  ROC curves값을 구하는 모듈이 내장됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "// 선형분류와 SVN 모델 \n",
    "val metrics = Seq(lrModel, svmModel).map { model =>\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "// naïve Bayes 모델\n",
    "val nbMetrics = Seq(nbModel).map{ model =>\n",
    "    val scoreAndLabels = nbData.map { point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "                                 \n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    ( model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC )\n",
    "}\n",
    "\n",
    "// decision tree\n",
    "val dtMetrics = Seq(dtModel).map{ model =>\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "                                 \n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "// 화면 출력\n",
    "val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
    "allMetrics.foreach{ case (m, pr, roc) =>\n",
    "    println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Improving model performance and tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Feature standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 많은 수의 모델들이 입력 데이터의 분포를 가정하고 있음. 예를 들면, 정규분포\n",
    "- 그러므로, 입력 데이터의 분포를 알아보는것이 필요\n",
    "- SparkMLlib에서는 RowMatrix 클래스를 사용해서 feature vectors( 독립변수들, X값들 )을 분포 matrix로 표현함.\n",
    "- RowMatrix 클래스를 matrix의 컬럼들의 통계량을 계산해주는 유용한 Utility을 제공함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "val vectors = data.map(lp => lp.features)\n",
    "val matrix = new RowMatrix(vectors)\n",
    "val matrixSummary = matrix.computeColumnSummaryStatistics()\n",
    "\n",
    "println(matrixSummary.mean)\n",
    "println(matrixSummary.min)\n",
    "println(matrixSummary.max)\n",
    "println(matrixSummary.variance)\n",
    "println(matrixSummary.numNonzeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평균( mean )과 분산( variance )을 보면, 두번째 특성치( feature )가 다른 특성치보다 매우 높게 나타나는것을 확인할 수 있으며, 이 특성치가 모델에서 많은 영향을 주고 있다고 판단할 수 있음.\n",
    "- 모든 특성치가 모델에 동일한 수준으로 영향을 주도록 하기 위해서, 평균이 0이고, 분산 1인  표준정규분포로 변환하는것이 필요\n",
    "![standardize](Classification_21.jpg \"standardize\")\n",
    "\n",
    "- Spark의 StandardScaler 클래스를 이용해서 vector들을 표준화함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n",
    "val scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features)))\n",
    "\n",
    "println(data.first.features)\n",
    "println(scaledData.first.features)\n",
    "\n",
    "println((0.789131 - 0.41225805299526636)/ math.sqrt(0.1097424416755897))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\n",
    "val lrTotalCorrectScaled = scaledData.map { point => \n",
    "    if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val lrAccuracyScaled = lrTotalCorrectScaled / numData\n",
    "val lrPredictionsVsTrue = scaledData.map { point =>\n",
    "    (lrModelScaled.predict(point.features), point.label)\n",
    "}\n",
    "\n",
    "val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\n",
    "val lrPr = lrMetricsScaled.areaUnderPR\n",
    "val lrRoc = lrMetricsScaled.areaUnderROC\n",
    "\n",
    "println(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr * 100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Additional features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산의 편리성을 위해서 카테고리 변수와 문자열 변수는 사용하지 않았음.\n",
    "- 카테고리 변수를 추가해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
    "val numCategories = categories.size\n",
    "\n",
    "println(categories)\n",
    "println(numCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dataCategories = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    val otherFeatures = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
    "    val features = categoryFeatures ++ otherFeatures\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "println(dataCategories.first)\n",
    "\n",
    "// 표준화 \n",
    "val scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features))\n",
    "val scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label, scalerCats.transform(lp.features)))\n",
    "\n",
    "println(dataCategories.first.features)\n",
    "println(scaledDataCats.first.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 카테고리 변수를 추가한 데이터로 예측모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)\n",
    "val lrTotalCorrectScaledCats = scaledDataCats.map { point =>\n",
    "    if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\n",
    "val lrPredictionsVsTrueCats = scaledDataCats.map { point => \n",
    "    (lrModelScaledCats.predict(point.features), point.label)\n",
    "}\n",
    "\n",
    "val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\n",
    "val lrPrCats = lrMetricsScaledCats.areaUnderPR\n",
    "val lrRocCats = lrMetricsScaledCats.areaUnderROC\n",
    "\n",
    "println(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats *100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Using the correct form of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각각의 학습모델마다 맞는 올바른 데이터 형식을 사용이 성능에 영향을 줌\n",
    "- 나이브베이즈 모델에 맞게 데이터 변환 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dataNB = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    LabeledPoint(label, Vectors.dense(categoryFeatures))\n",
    "}\n",
    "println(dataNB.first)\n",
    "\n",
    "val nbModelCats = NaiveBayes.train(dataNB)\n",
    "val nbTotalCorrectCats = dataNB.map { point =>\n",
    "    if (nbModelCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val nbAccuracyCats = nbTotalCorrectCats / numData\n",
    "val nbPredictionsVsTrueCats = dataNB.map { point =>\n",
    "    (nbModelCats.predict(point.features), point.label)\n",
    "}\n",
    "\n",
    "val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\n",
    "val nbPrCats = nbMetricsCats.areaUnderPR\n",
    "val nbRocCats = nbMetricsCats.areaUnderROC\n",
    "println(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Tuning model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1. Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression과 SVM은 파라메터가 동일\n",
    "- stochastic gradient descent (SGD)의 최적화 기술을 동일함.\n",
    "- 차이점은 다른 loss function 를 갖음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionWithSGD private (\n",
    "   private var stepSize: Double,\n",
    "   private var numIterations: Int, \n",
    "   private var regParam: Double,\n",
    "   private var miniBatchFraction: Double)\n",
    "   extends GeneralizedLinearAlgorithm[LogisticRegressionModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SparkML의 logistic regression에 대한 생성자 정의는 위와 같음.\n",
    "- stepSize, numIterations, regParam, and miniBatchFraction는 필수\n",
    "- regParam 생략 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "private val gradient = new LogisticGradient()\n",
    "private val updater = new SimpleUpdater()\n",
    "override val optimizer = new GradientDescent(gradient, updater).setStepSize(stepSize).setNumIterations(numIterations).setRegParam(regParam).setMiniBatchFraction(miniBatchFraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 코드는 Gradient, Updater, and Optimizer을 초기설정하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.optimization.Updater\n",
    "import org.apache.spark.mllib.optimization.SimpleUpdater\n",
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
    "import org.apache.spark.mllib.classification.ClassificationModel\n",
    "\n",
    "def trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\n",
    "    val lr = new LogisticRegressionWithSGD\n",
    "    lr.optimizer.setNumIterations(numIterations).setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
    "    lr.run(input)\n",
    "}\n",
    "\n",
    "def createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\n",
    "   val scoreAndLabels = data.map { point =>\n",
    "       (model.predict(point.features), point.label)\n",
    "   }\n",
    "   val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "   (label, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "scaledDataCats.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterations 변경에 따른 성능 영향 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val iterResults = Seq(1, 5, 10, 50).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n",
    "    createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
    "}\n",
    "\n",
    "iterResults.foreach { \n",
    "    case (param, auc) => println(f\"$param, AUC =${auc * 100}%2.2f%%\") \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step size변경에 따른 성능 영향 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n",
    "    createMetrics(s\"$param step size\", scaledDataCats, model)\n",
    "}\n",
    "\n",
    "stepResults.foreach { \n",
    "    case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization( 정규화 ) 변경에 따른 성능 영향 확인\n",
    "- updater 클래스는 SparkML에서 구현한 Regularization 모듈\n",
    "- Regularization는 많은 변수를 갖는 복잡한 모델에 효과적으로 패널티를 주므로 해서 학습할때 과학습( over-fitting )을 피하는데 도움을 줌.\n",
    "- 변수가 많을때( feature dimension is very high )에 중요한 요소임.\n",
    "- regularization의 종류\n",
    "- 1) SimpleUpdater: no regularization\n",
    "- 2) SquaredL2Updater: the squared L2-norm of the weight vector\n",
    "- 3) L1Updater: L1-norm of the weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
    "}\n",
    "\n",
    "regResults.foreach { \n",
    "    case (param, auc) => println(f\"$param, AUC =${auc * 100}%2.2f%%\") \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나무의 깊이(tree depth) 와 불순도( impurity )를 파라마터로 설정\n",
    "- 나무의 깊이가 깊을수록 학습데이터에 대해서는 성능이 좋아지만,, 과학습( over-fitting )이 되어 예측력은 떨어짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Gini\n",
    "def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int,impurity: Impurity) = {\n",
    "    DecisionTree.train(input, Algo.Classification, impurity,maxDepth)\n",
    "}\n",
    "\n",
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
    "    val model = trainDTWithParams(data, param, Entropy)\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "                                                       \n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "dtResultsEntropy.foreach { \n",
    "    case (param, auc) => println(f\"$param,AUC = ${auc * 100}%2.2f%%\") \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3. The naïve Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나이브베이즈 모델의 파라메타는 lambda 임.\n",
    "- lambda는 데이터셋에서 분류와 변수가 같이 일어나지 않는 상황을 처리하기 위해서 추가적인 smoothing을 컨트롤 함.\n",
    "- This parameter controls additive smoothing, which handles the case when a class and feature value do not occur together in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n",
    "    val nb = new NaiveBayes\n",
    "    nb.setLambda(lambda)\n",
    "    nb.run(input)\n",
    "}\n",
    "\n",
    "val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainNBWithParams(dataNB, param)\n",
    "    val scoreAndLabels = dataNB.map { point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }                                                      \n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param lambda\", metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "nbResults.foreach { \n",
    "    case (param, auc) => println(f\"$param, AUC =${auc * 100}%2.2f%%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 121)\n",
    "val train = trainTestSplit(0)\n",
    "val test = trainTestSplit(1)\n",
    "\n",
    "val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n",
    "    val model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", test, model)\n",
    "}\n",
    "\n",
    "regResultsTest.foreach { \n",
    "    case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나무모형에서 cross-validation시 예측력의 변화 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
    "    val model = trainDTWithParams(train, param, Entropy)\n",
    "    val scoreAndLabels = test.map { point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "                                                       \n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "dtResultsEntropy.foreach { \n",
    "    case (param, auc) => println(f\"$param,AUC = ${auc * 100}%2.2f%%\") \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
