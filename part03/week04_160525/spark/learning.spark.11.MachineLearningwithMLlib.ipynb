{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f86fd6a9dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11장 Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - MLib의 디장인과 철학은 단순 : RDD를 사용하는 ML함수 집합\n",
    " \n",
    " - ML을 이용한 text classification 작업의 예\n",
    "     - 1) 메시지를 표현하는 문자열들이 담긴 RDD를 준비 \n",
    "     - 2) MLlib 특성 추출 알고리즘들 중 하나를 써서 문자열을 수치화된 특성으로 변환을 통한 결과 벡터들의 RDD리턴\n",
    "     - 3) 벡터의 RDD에 분류 알고리즘을 호출, 신규 데이터들을 분류하는데 쓰일 모델 객체 리턴\n",
    "     - 4) MLlib평가 함수들 중 하나를 써서 테스트 데이터 세트에 모델을 적용, 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시스템 요구 사항\n",
    "- MLlib은 시스템에 몇몇 선형대수 라이브러리 설치를 필요로 함. 만약 해당 오류 발생시 우선 OS에 gfortran 실행 라이브러리 설치\n",
    "- Dependencies\n",
    "   - 스칼라, 자바 : breeze \n",
    "   - 파이션 : numpy\n",
    "- 우리의 환경 설정 : docker pull jupyter/all-spark-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 머신러닝의 기초\n",
    "- 머신러닝 알고리즘은 훈련 데이터를 통한 알고리즘의 수학적 결과들을 토대로 예측, 분류등에 대한 결정을 내리려고 시도함.\n",
    "- 머신러닝의 일반적인 단계 : \n",
    "    - 문제정의 -> 변수추출 -> 정제 및 변환 -> 분석 -> 측정\n",
    "    \n",
    "![](spark11_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스팸분류 예제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-05-23 01:33:04--  https://raw.githubusercontent.com/biospin/BigBio/master/part03/week04_160525/spark/spam.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 103.245.222.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|103.245.222.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16144 (16K) [text/plain]\n",
      "Saving to: ‘spam.txt’\n",
      "\n",
      "spam.txt            100%[=====================>]  15.77K  --.-KB/s   in 0.03s  \n",
      "\n",
      "2016-05-23 01:33:14 (458 KB/s) - ‘spam.txt’ saved [16144/16144]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/biospin/BigBio/master/part03/week04_160525/spark/spam.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-05-23 01:33:47--  https://raw.githubusercontent.com/biospin/BigBio/master/part03/week04_160525/spark/normal.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 103.245.222.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|103.245.222.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17295 (17K) [text/plain]\n",
      "Saving to: ‘normal.txt’\n",
      "\n",
      "normal.txt          100%[=====================>]  16.89K  --.-KB/s   in 0.06s  \n",
      "\n",
      "2016-05-23 01:33:53 (276 KB/s) - ‘normal.txt’ saved [17295/17295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/biospin/BigBio/master/part03/week04_160525/spark/normal.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\r\n",
      "-rw-r--r-- 1 jovyan users  4215 May 23 01:34 learning.spark.11.MachineLearningwithMLlib.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users 17295 May 23 01:33 normal.txt\r\n",
      "-rw-r--r-- 1 jovyan users 16144 May 23 01:33 spam.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint \n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = sc.textFile( \"spam.txt\" )\n",
    "normal = sc.textFile( \"normal.txt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이메일 문자열을 10000개의 특징의 벡터로 매핑하기 위한 HashingTF 인스턴스를 만듬.\n",
    "tf = HashingTF( numFeatures=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 각 이메일을 단어로 구분하고, 각 단어는 하나의 특성에 패밍\n",
    "spamFeatures = spam.map( lambda email : tf.transform( email.split(\" \") )  )\n",
    "normalFeatures = normal.map( lambda email : tf.transform( email.split(\" \") )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[15] at union at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 양성( spam )과 음성( 일반 ) 예제를 위한 LabeledPoint( 1, features ) 데이터세을 만듬.\n",
    "positiveExamples = spamFeatures.map( lambda features : LabeledPoint( 1, features ) ) \n",
    "negativeExamples = normalFeatures.map( lambda features : LabeledPoint( 0, features ) ) \n",
    "trainingData = positiveExamples.union( negativeExamples   )\n",
    "trainingData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD 알고리즘으로 로직스틱 회귀를 수행\n",
    "model = LogisticRegressionWithSGD.train(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for positive test example: 1\n",
      "Prediction for negative test example: 0\n"
     ]
    }
   ],
   "source": [
    "# 양성예제와 음성예제를 테스트한다.\n",
    "# 우선, 벡터들을 얻기 위해 동일한 HashingTF 특징 변형을 적용한 후 모델을 적용함.\n",
    "posTest = tf.transform( \"Why not grab out amazing Python Data Bundle for only $50. Pick up our Python Data Bundle for $50 Purchase Bundle View all eBooks & Videos\".split(\" \") )\n",
    "negTest = tf.transform( \"Research Center developed a highly optimized matrix factorization system to accelerate recommendations for e-commerce sites\".split(\" \") )\n",
    "\n",
    "\n",
    "print \"Prediction for positive test example:\", model.predict(posTest)\n",
    "print \"Prediction for negative test example:\", model.predict(negTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 타입\n",
    "\n",
    "- MLlib에서의 특정 타임들의 패키지 위치\n",
    "    - 자바, 스칼라 : org.apache.spark.mllib\n",
    "    - 파이션 : pyspark.mllib\n",
    "    \n",
    " - 주요 데이터 타입\n",
    "    - Local Vector : 수학적인 의미의 벡터, dense vector와 sparse vector 모두 지원\n",
    "    - Labeled point : 분류나 회귀같은 지도학습 알고리즘을 위한 Labeled Point\n",
    "    - Rating : 사용자 상품 순위, 상품 추천을 위한 mllib.recommendation 패키지에서 제공\n",
    "    - 기타 여러가지 타입이 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 고밀도 백터< 1.0, 2.0, 3.0 > 을 만듬\n",
    "denseVec1 = array( [1.0, 2.0, 3.0] ) # numpy 배열은 MLlib에 직접 전달\n",
    "denseVec2 = Vectors.dense( [1.0, 2.0, 3.0] ) # Vector 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 정밀도 벡터< 1.0, 0.0, 2.0, 0.0 > 을 만듬. 벡터의 크기는 4\n",
    "sparseVec1 = Vectors.sparse(4, { 0:1.0, 2:2.0 } )\n",
    "sparseVec2 = Vectors.sparse(4, [0, 2],  [1.0, 2.0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특성추출\n",
    "    - TF-IDF : 문서에 대한 특성을 추출할때 사용하고, 단어 빈도수와 역문서 빈도를 곱한 값\n",
    "    - IDF : 전체문서의 수(분자)를 해당 단어가 포함된 문서의 수(분모)로 나눈 값에 로그를 취한 값\n",
    "    - TF-IDF의 의미는 자주 나오는 단어도 중요하지만, 전체문서에서 드물게 나온 단어가 더 중요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: Why not grab out amazing  : No such file or directory\n",
      "cat: Research Center developed a highly optimize: No such file or directory\n",
      "cat: Pick up our Python Data Bundle for: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cat \"Why not grab out amazing  \" > data/aaa.txt\n",
    "! cat \"Research Center developed a highly optimize\" > data/bbb.txt\n",
    "! cat \"Pick up our Python Data Bundle for\" > data/ccc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "-rw-r--r-- 1 jovyan users 0 May 23 02:37 aaa.txt\r\n",
      "-rw-r--r-- 1 jovyan users 0 May 23 02:37 bbb.txt\r\n",
      "-rw-r--r-- 1 jovyan users 0 May 23 02:37 ccc.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l  /home/jovyan/work/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10000, {3065: 1.0, 7741: 2.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = \"Hello Hello world\" \n",
    "words = sentense.split()\n",
    "tf = HashingTF( 10000 )\n",
    "tf.transform( words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,[],[])\n",
      "(10000,[],[])\n",
      "(10000,[],[])\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.wholeTextFiles(\"/home/jovyan/work/data\").map( lambda(name ,text) : text.split()  )\n",
    "tfVectors = tf.transform( rdd )\n",
    "for x in tfVectors.collect():\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정량화\n",
    "\n",
    "- 각각의 특성치들이 스케일이 다르면, 큰 값을 갖는 특성치가 더 많이 적용하게 됨.\n",
    "- 모든 특성들에 동일하게 고려하기 위해서  정량화가 필요함.\n",
    "- mllib.feature의 StandardScaler( 평균 : 0,  표준편차 1로 정량화 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.707106781187,0.707106781187,0.0]\n",
      "[0.707106781187,-0.707106781187,0.0]\n"
     ]
    }
   ],
   "source": [
    "vectors = [ Vectors.dense([-2, 5.0, 1.0]), Vectors.dense([2.0, 0.0, 1.0])  ]\n",
    "dataset = sc.parallelize(vectors)\n",
    "scaler = StandardScaler( withMean=True, withStd=True)\n",
    "model = scaler.fit( dataset )\n",
    "result = model.transform( dataset )\n",
    "for x in result.collect():\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 정규화 \n",
    "\n",
    "- 어떤 상황에서는 벡터를 길이 1로 정규화하는 것이 입력데이터를 준비하는데 도움.\n",
    "- Normailizer클래스가 이를 담당하여 Normailizer().transform(rdd)를 호출하여 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 신경망 네트워크 기반 문자열의 특징을 판별하는 알고리즘."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 통계 \n",
    "\n",
    "- mllib.stat.Statisics 클래스의 메소드\n",
    "- Statisics.coStats( rdd ) \n",
    "    - 벡터의 RDD의 통계적인 요약을 계산하며, 최소값, 최대값, 평균, 분산등\n",
    "- Statisics.corr( rdd, method ) \n",
    "    - 벡터의 RDD의 컬럼들ㅇ 사이에 상관관계 행렬을 계산, 피어슨이나 스피어만 상관관계 중 하나를 사용\n",
    "- Statisics.chiSqTest( rdd ) \n",
    "    - LabeledPoint 형태로 추출된 Feature에 대한 피어슨 독립성 검증 제공\n",
    "    - 결과로 유의확율값, t-통계값 , 각 변수의 자유도를 배열 형태로 제공\n",
    "기타 : mean, stdev, sum 함수 등 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
