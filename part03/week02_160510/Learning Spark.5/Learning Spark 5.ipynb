{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d96f2b10d1e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minputFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Prt5.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import StringIO\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "inputFile = './Prt5.txt'\n",
    "#sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cat Prt5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadRecord(line):\n",
    "    ''' Parse a CSV line'''\n",
    "    #print line\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader  = csv.DictReader(input, fieldnames= fields, delimiter = '\\t')\n",
    "    return reader.next()\n",
    "\n",
    "raw_input = sc.textFile(inputFile)\n",
    "inputHeader = raw_input.first()\n",
    "fields = [el for el in inputHeader.split('\\t')]\n",
    "inputNoheader = raw_input.subtract( sc.parallelize([raw_input.first()]))\n",
    "inputNoheader.count()\n",
    "data = inputNoheader.map(loadRecord)\n",
    "\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.filter(lambda x: x).map(lambda x: json.dumps(x)).saveAsTextFile('./Pr5.json.Gzip', compressionCodecClass = \"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "#data.filter(lambda x: x).map(lambda x: json.dumps(x)).saveAsTextFile('./Pr5.json.Lzo', compressionCodecClass = \"com.hadoop.compression.lzo.LzoCodec\")\n",
    "\n",
    "#%cat ./Pr5.json.Gzip/\n",
    "%ls ./Pr5.json.Gzip/\n",
    "#%ls ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_json = sc.textFile('./Pr5.json.Gzip/part-00000.gz').map(lambda x: json.loads(x))\n",
    "data_json = sc.textFile('./Pr5.json/')\n",
    "\n",
    "data_json.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeRecords(records):\n",
    "    \"\"\"Write out CSV lines\"\"\"\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fields)#[\"name\", \"favoriteAnimal\", \"c\", \"d\"]\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "data.mapPartitions(writeRecords).saveAsTextFile('Prt5csv.csv')\n",
    "\n",
    "%cat ./Prt5csv.csv/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_seq = sc.parallelize([(\"Panda\", 3), (\"Kay\", 6), (\"Snail\", 2)])\n",
    "data_seq.saveAsSequenceFile('./data_seq')\n",
    "\n",
    "%cat ./data_seq/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_seq_rel= sc.sequenceFile(\"./data_seq/\", \"org.apache.hadoop.io.Text\", \"org.apache.hadoop.io.IntWritable\")\n",
    "data_seq_rel\n",
    "\n",
    "data_seq_rel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Prt5.txt\n",
    "Name\tCountry\tEmail\tOS\n",
    "John\tUSA\tjohn@gmail.com\tWindows\n",
    "Smith\tCanada\tsmith@yahoo.com\tLinux\n",
    "Yan\tChina\tyan@alibaba.com\tWindows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
